# Instruct QnA Fine Tuning - Google Flan T5 Large - LLM QLoRA PEFT - Open Orca Dataset


Project Title: Fine-tuning Large Language Model for Instruct Learning QnA.

Objective: Efficiently fine-tuned Flan T5 Large Model for specific tasks using Parameter Efficient Finetuning and Transfer Learning techniques based on explicit instructions from Open Orca Dataset.

Dataset: Modified prompts and preprocessed data for encoder-decoder setup, covering diverse tasks (MCQ, Reasoning, QnA etc.) while adhering to the model's constraints.

Implementation Techniques: Utilized Flan T5 Large Model, integrated QLoRA Layer for quantization efficiency gains, leading to reduced computational costs, space-efficient storage, achieved code efficiency for multi-epoch training and inference on Kaggle single GPU.

Results: Attained notable results on 75k data points, showcasing the model's efficacy, with ongoing project expansion to accommodate more data points.
