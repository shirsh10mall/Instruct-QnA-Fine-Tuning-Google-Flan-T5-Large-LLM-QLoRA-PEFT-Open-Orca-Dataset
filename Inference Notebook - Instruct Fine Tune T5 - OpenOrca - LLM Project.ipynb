{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment Setup","metadata":{}},{"cell_type":"code","source":"import sys\n\nif 'kaggle_web_client' in sys.modules:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    HUGGINGFACE_API_KEY = user_secrets.get_secret(\"SKT_HUGGINGFACE_API_KEY\")\n    # WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\nelse:\n    from dotenv import load_dotenv\n    import os\n    load_dotenv()\n    HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n    # WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:39:47.268568Z","iopub.execute_input":"2023-07-31T05:39:47.269265Z","iopub.status.idle":"2023-07-31T05:39:47.434888Z","shell.execute_reply.started":"2023-07-31T05:39:47.269227Z","shell.execute_reply":"2023-07-31T05:39:47.433670Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Installing Required Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate evaluate rouge_score\n!pip install -q datasets loralib einops\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:28:00.160925Z","iopub.execute_input":"2023-07-31T05:28:00.161288Z","iopub.status.idle":"2023-07-31T05:30:17.723530Z","shell.execute_reply.started":"2023-07-31T05:28:00.161256Z","shell.execute_reply":"2023-07-31T05:30:17.722172Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Huggingface login","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nfrom huggingface_hub import login\nlogin(token=HUGGINGFACE_API_KEY)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-31T05:40:02.241552Z","iopub.execute_input":"2023-07-31T05:40:02.241965Z","iopub.status.idle":"2023-07-31T05:40:02.272769Z","shell.execute_reply.started":"2023-07-31T05:40:02.241931Z","shell.execute_reply":"2023-07-31T05:40:02.271753Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# imports\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nimport evaluate\n\nimport torch\nimport copy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom accelerate import Accelerator\n\nimport bitsandbytes as bnb\nfrom peft import PeftModel, PeftConfig, prepare_model_for_kbit_training, LoraConfig, get_peft_model\n\nfrom transformers import AutoTokenizer, get_scheduler, BitsAndBytesConfig, GenerationConfig\nfrom transformers import  AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:30:18.019837Z","iopub.execute_input":"2023-07-31T05:30:18.020151Z","iopub.status.idle":"2023-07-31T05:30:34.651948Z","shell.execute_reply.started":"2023-07-31T05:30:18.020120Z","shell.execute_reply":"2023-07-31T05:30:34.650989Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:30:34.654270Z","iopub.execute_input":"2023-07-31T05:30:34.654610Z","iopub.status.idle":"2023-07-31T05:30:34.659314Z","shell.execute_reply.started":"2023-07-31T05:30:34.654578Z","shell.execute_reply":"2023-07-31T05:30:34.658406Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data, tokenizer and Peft fine-tunned model","metadata":{}},{"cell_type":"code","source":"# dataset = load_dataset(\"SKT27182/Preprocessed_OpenOrca\", streaming=True)\ndataset = load_dataset(\"shirsh10mall/LLM_Instruct_Learning_Project_Preprocessed_Tokenized_Open_Orca_Dataset_Flan_T5\", streaming=True)\n\nn_samples = 300\n\ndata = { \"input_ids\":[], \"attention_mask\":[], \"labels\": [], \"system_prompt\":[], \"question\":[], \"response\":[]}\n\nfor i, sample in enumerate(dataset[\"train\"]):\n    if i >= n_samples:\n        break\n    for key, value in sample.items():\n        if key in data.keys():\n            data[key].append(value)\n        \nopen_orca = Dataset.from_dict(data)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:30:37.318158Z","iopub.execute_input":"2023-07-31T05:30:37.318542Z","iopub.status.idle":"2023-07-31T05:30:50.254606Z","shell.execute_reply.started":"2023-07-31T05:30:37.318510Z","shell.execute_reply":"2023-07-31T05:30:50.253622Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# split the data for training and validation purpose\n# open_orca = open_orca.train_test_split(train_size=0.9, seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:30:50.256696Z","iopub.execute_input":"2023-07-31T05:30:50.257040Z","iopub.status.idle":"2023-07-31T05:30:50.261458Z","shell.execute_reply.started":"2023-07-31T05:30:50.257007Z","shell.execute_reply":"2023-07-31T05:30:50.260558Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# peft_model_id = \"shirsh10mall/First_LLM_Project\"\n# # peft_model_id = \"SKT27182/Qlora\"\n\n# config = PeftConfig.from_pretrained(peft_model_id)\n# model = AutoModelForSeq2SeqLM.from_pretrained( config.base_model_name_or_path, return_dict=True, load_in_8bit=True, \n#                                                  device_map={\"\":0}, trust_remote_code=True, )\n\n# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# # Load the Lora model\n# model = PeftModel.from_pretrained(model, peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:30:50.262980Z","iopub.execute_input":"2023-07-31T05:30:50.263609Z","iopub.status.idle":"2023-07-31T05:30:50.293676Z","shell.execute_reply.started":"2023-07-31T05:30:50.263576Z","shell.execute_reply":"2023-07-31T05:30:50.292646Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ncheckpoint = \"google/flan-t5-large\"\nlora_config = LoraConfig( r=8, lora_alpha=16, target_modules=['q', 'v'], lora_dropout=0.05, bias=\"none\", task_type= \"SEQ_2_SEQ_LM\" )\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, quantization_config=bnb_config, device_map={\"\":0}, trust_remote_code=True)\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:30:50.668376Z","iopub.execute_input":"2023-07-31T05:30:50.668950Z","iopub.status.idle":"2023-07-31T05:31:18.446998Z","shell.execute_reply.started":"2023-07-31T05:30:50.668917Z","shell.execute_reply":"2023-07-31T05:31:18.446013Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d16d5483cbc41b79eca578fcc36335c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e97000eea74977a985c66dda322612"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fff5c7b3a313481aa80de7493f26acdf"}},"metadata":{}}]},{"cell_type":"code","source":"import copy\ntuned_model  = copy.deepcopy(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:20:17.565772Z","iopub.execute_input":"2023-07-31T05:20:17.566146Z","iopub.status.idle":"2023-07-31T05:20:17.978453Z","shell.execute_reply.started":"2023-07-31T05:20:17.566116Z","shell.execute_reply":"2023-07-31T05:20:17.977283Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# def load_checkpoint(model,filepath):\n#     checkpoint = torch.load(filepath)\n#     model.load_state_dict(checkpoint['state_dict'])\n# #     for parameter in model.parameters():\n# #         parameter.requires_grad = False\n# #     model.eval()\n#     return model\n\n# tuned_model = load_checkpoint(tuned_model,'/kaggle/input/llm-google-flan-t5-large-qlora-fine-tuning/LLM_Project_model_dict.pth.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:25:14.772891Z","iopub.execute_input":"2023-07-31T05:25:14.773908Z","iopub.status.idle":"2023-07-31T05:25:14.782633Z","shell.execute_reply.started":"2023-07-31T05:25:14.773867Z","shell.execute_reply":"2023-07-31T05:25:14.777591Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.peft_config","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='google/flan-t5-large', revision=None, task_type='SEQ_2_SEQ_LM', inference_mode=False, r=8, target_modules=['q', 'v'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)}"},"metadata":{}}]},{"cell_type":"code","source":"model.model?","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:32:32.930403Z","iopub.execute_input":"2023-07-31T05:32:32.931106Z","iopub.status.idle":"2023-07-31T05:32:32.959765Z","shell.execute_reply.started":"2023-07-31T05:32:32.931061Z","shell.execute_reply":"2023-07-31T05:32:32.958661Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mType:\u001b[0m           T5ForConditionalGeneration\n\u001b[0;31mString form:\u001b[0m   \nT5ForConditionalGeneration(\n           (shared): Embedding(32128, 1024)\n           (encoder): T5Stack(\n           (embed_t <...> p=0.1, inplace=False)\n           )\n           (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n           )\n\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\n\u001b[0;31mDocstring:\u001b[0m     \nT5 Model with a `language modeling` head on top.\n\nThe T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text\nTransformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a\ntext-to-text denoising generative setting.\n\nThis model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\nlibrary implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\netc.)\n\nThis model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\nUse it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\nand behavior.\n\nParameters:\n    config ([`T5Config`]): Model configuration class with all the parameters of the model.\n        Initializing with a config file does not load the weights associated with the model, only the\n        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule."},"metadata":{}}]},{"cell_type":"code","source":"model?","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:31:26.754204Z","iopub.execute_input":"2023-07-31T05:31:26.754938Z","iopub.status.idle":"2023-07-31T05:31:26.781074Z","shell.execute_reply.started":"2023-07-31T05:31:26.754902Z","shell.execute_reply":"2023-07-31T05:31:26.780059Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mSignature:\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mType:\u001b[0m        PeftModelForSeq2SeqLM\n\u001b[0;31mString form:\u001b[0m\nPeftModelForSeq2SeqLM(\n           (base_model): LoraModel(\n           (model): T5ForConditionalGeneration(\n           <...> se)\n           )\n           (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n           )\n           )\n           )\n\u001b[0;31mFile:\u001b[0m        /opt/conda/lib/python3.10/site-packages/peft/peft_model.py\n\u001b[0;31mDocstring:\u001b[0m  \nPeft model for sequence-to-sequence language modeling.\n\nArgs:\n    model ([`~transformers.PreTrainedModel`]): Base transformer model.\n    peft_config ([`PeftConfig`]): Peft config.\n\n\nExample:\n\n    ```py\n    >>> from transformers import AutoModelForSeq2SeqLM\n    >>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n\n    >>> config = {\n    ...     \"peft_type\": \"LORA\",\n    ...     \"task_type\": \"SEQ_2_SEQ_LM\",\n    ...     \"inference_mode\": False,\n    ...     \"r\": 8,\n    ...     \"target_modules\": [\"q\", \"v\"],\n    ...     \"lora_alpha\": 32,\n    ...     \"lora_dropout\": 0.1,\n    ...     \"fan_in_fan_out\": False,\n    ...     \"enable_lora\": None,\n    ...     \"bias\": \"none\",\n    ... }\n\n    >>> peft_config = get_peft_config(config)\n    >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n    >>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)\n    >>> peft_model.print_trainable_parameters()\n    trainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n    ```"},"metadata":{}}]},{"cell_type":"code","source":"model.model.push_to_hub?","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:36:41.021058Z","iopub.execute_input":"2023-07-31T05:36:41.021460Z","iopub.status.idle":"2023-07-31T05:36:41.047344Z","shell.execute_reply.started":"2023-07-31T05:36:41.021430Z","shell.execute_reply":"2023-07-31T05:36:41.046213Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mSignature:\u001b[0m\n\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrepo_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_temp_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcommit_message\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mprivate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_shard_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'10GB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcreate_pr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msafe_serialization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mdeprecated_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nUpload the model file to the ðŸ¤— Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the repository you want to push your model to. It should contain your organization name\n        when pushing to a given organization.\n    use_temp_dir (`bool`, *optional*):\n        Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n        Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n    commit_message (`str`, *optional*):\n        Message to commit while pushing. Will default to `\"Upload model\"`.\n    private (`bool`, *optional*):\n        Whether or not the repository created should be private.\n    token (`bool` or `str`, *optional*):\n        The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n        when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n        is not specified.\n    max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n        Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n        will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n        by a unit (like `\"5MB\"`).\n    create_pr (`bool`, *optional*, defaults to `False`):\n        Whether or not to create a PR with the uploaded files or directly commit.\n    safe_serialization (`bool`, *optional*, defaults to `False`):\n        Whether or not to convert the model weights in safetensors format for safer serialization.\n\nExamples:\n\n```python\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n\n# Push the model to your namespace with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"my-finetuned-bert\")\n\n# Push the model to an organization with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"huggingface/my-finetuned-bert\")\n```\n\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\n\u001b[0;31mType:\u001b[0m      method"},"metadata":{}}]},{"cell_type":"code","source":"model.model.push_to_hub(\"shirsh10mall/First_LLM_Project\")   # full model LoRA + Base","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:33:33.149273Z","iopub.execute_input":"2023-07-31T05:33:33.150053Z","iopub.status.idle":"2023-07-31T05:34:06.782979Z","shell.execute_reply.started":"2023-07-31T05:33:33.150005Z","shell.execute_reply":"2023-07-31T05:34:06.781833Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1731: UserWarning: You are calling `save_pretrained` to a 8-bit converted model you may likely encounter unexepected behaviors. If you want to save 8-bit models, make sure to have `bitsandbytes>0.37.2` installed.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.41G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee34618f9189465eb4c463cc9306c37e"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shirsh10mall/First_LLM_Project/commit/9835b6b83ae46184d3093246f6e44e4391bfc1e2', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='9835b6b83ae46184d3093246f6e44e4391bfc1e2', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"SKT27182/Qlora\", use_authentication=True)  # LoRA layer","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:40:12.864810Z","iopub.execute_input":"2023-07-31T05:40:12.865237Z","iopub.status.idle":"2023-07-31T05:40:13.217353Z","shell.execute_reply.started":"2023-07-31T05:40:12.865202Z","shell.execute_reply":"2023-07-31T05:40:13.216274Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/SKT27182/Qlora/commit/f6908d4795f2f095f20cc6cdc2e62fd429693cf2', commit_message='Upload model', commit_description='', oid='f6908d4795f2f095f20cc6cdc2e62fd429693cf2', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"new_model = AutoModelForSeq2SeqLM.from_pretrained(\"shirsh10mall/First_LLM_Project\", trust_remote_code=True)  # this is ignoring the LoRA layer's weights","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:34:57.588002Z","iopub.execute_input":"2023-07-31T05:34:57.588432Z","iopub.status.idle":"2023-07-31T05:35:17.437895Z","shell.execute_reply.started":"2023-07-31T05:34:57.588399Z","shell.execute_reply":"2023-07-31T05:35:17.436874Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef3e4d24d73498bbdb2cf5e0d6297fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.41G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d2be71c999b40c58d0299c9c7a6998f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at shirsh10mall/First_LLM_Project were not used when initializing T5ForConditionalGeneration: ['decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.12.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.13.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.16.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.13.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.17.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.21.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.21.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.19.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.22.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.23.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.16.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.15.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.22.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.21.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.19.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.23.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.14.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.12.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.23.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.13.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.21.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.13.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.14.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.20.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.19.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.23.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.22.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.22.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.18.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.20.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.23.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.17.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.16.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.17.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.19.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.17.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.22.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.13.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.18.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.18.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.22.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.23.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.20.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.19.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.19.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.17.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.21.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.21.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.12.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.13.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.19.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.17.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.23.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.12.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.17.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.22.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.20.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.16.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.13.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.23.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.22.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.16.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.14.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.17.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.23.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.16.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.13.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.16.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.15.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.12.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.15.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.14.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.21.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.14.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.12.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.15.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.15.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.22.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.21.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.15.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.20.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.12.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.14.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.15.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.23.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.16.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.15.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.18.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.21.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.15.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.14.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.20.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.13.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.15.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.20.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.18.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.17.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.21.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.18.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.12.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.13.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.19.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.18.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.12.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.23.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.19.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.18.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.18.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.17.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.21.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.14.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.12.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.15.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.16.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.19.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.22.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.19.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.16.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.12.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.16.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.19.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.20.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.12.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.13.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.21.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.13.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.14.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.22.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.18.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.17.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.22.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.20.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.20.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.17.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.14.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.14.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.20.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.18.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.16.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.14.layer.1.EncDecAttention.v.lora_A.default.weight', 'encoder.block.20.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.18.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.23.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight']\n- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb5574d8b71f4338accad8f7f3d8b85b"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_model_weights(model1, model2, tolerance=0):\n    state_dict1 = model1.state_dict()\n    state_dict2 = model2.state_dict()\n\n    # Check if the keys of the state dictionaries match\n    if set(state_dict1.keys()) != set(state_dict2.keys()):\n        return False\n\n    # Check if the values of the state dictionaries match for each key\n    for key in state_dict1.keys():\n        tensor1 = state_dict1[key]\n        tensor2 = state_dict2[key]\n\n        # Check if tensors have the same shape\n        if tensor1.shape != tensor2.shape:\n            return False\n\n        # Check if tensors have the same values within the specified tolerance\n        if not torch.allclose(tensor1, tensor2, atol=tolerance):\n            return False\n\n\n    return True\n\nare_models_equal = compare_model_weights(model, tuned_model)\n\nif are_models_equal:\n    print(\"The models have the same weights.\")\nelse:\n    print(\"The models have different weights.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:35:18.582036Z","iopub.execute_input":"2023-07-31T05:35:18.582393Z","iopub.status.idle":"2023-07-31T05:35:18.624030Z","shell.execute_reply.started":"2023-07-31T05:35:18.582361Z","shell.execute_reply":"2023-07-31T05:35:18.622967Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"The models have different weights.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.base_model.model.lm_head.weight.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T04:31:45.499331Z","iopub.execute_input":"2023-07-31T04:31:45.499676Z","iopub.status.idle":"2023-07-31T04:31:45.507336Z","shell.execute_reply.started":"2023-07-31T04:31:45.499650Z","shell.execute_reply":"2023-07-31T04:31:45.506269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zero shot Prediction\n","metadata":{}},{"cell_type":"code","source":"def analyse_zero_shot_model(data, indx, tokenizer, model, peft=False, max_tokens=200):\n    \n    prompt = data[indx][\"system_prompt\"]\n\n    question = data[indx][\"question\"]\n    \n    response = data[indx][\"response\"]\n    \n    print(\"Prompt:\")\n    print(prompt)\n    print()\n    \n    print('Question:')\n    print(question)\n    print()\n    \n    print(\"Response\")\n    print(response)\n    print()\n    \n    tokenized_input = tokenizer( prompt + \" \" + question, padding=False, truncation=True, return_tensors=\"pt\")\n    \n    print(\"Tokenized Input: Prompt + Question\")\n    print(tokenized_input)\n    print()\n    \n    print(tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0]))\n    print()\n    \n    print(\"Zero-shot Prediction:\")\n    \n    device = model.device\n    \n    if peft:\n    \n        predicted_response = model.generate(input_ids = tokenized_input[\"input_ids\"].to(device), generation_config=GenerationConfig(max_new_tokens=max_tokens, num_beams=100))\n    else:\n        predicted_response = model.generate(input_ids = tokenized_input[\"input_ids\"].to(device))\n    \n    \n    predicted_output = tokenizer.decode(predicted_response[0], skip_special_tokens=True)\n    print(predicted_output)\n    \n    \nindex = 34\n    \nanalyse_zero_shot_model(open_orca, index, tokenizer, model, peft=True, max_tokens=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 110\nanalyse_zero_shot_model(open_orca, index, tokenizer, model, peft=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 899\nanalyse_zero_shot_model(open_orca, index, tokenizer, model, peft=True, max_tokens=300)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 655\nanalyse_zero_shot_model(open_orca, index, tokenizer, model, peft=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}}]}