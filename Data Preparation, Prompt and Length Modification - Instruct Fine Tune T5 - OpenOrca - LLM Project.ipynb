{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzkSo0VJiEMt"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPHIx3AsDeD5"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPCv9YQTiN3f"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frJuXR-WDgkz"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"Open-Orca/OpenOrca\")\n",
        "data = data['train']\n",
        "# data = data.to_pandas()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rME9dL7qfqbm"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_path = '/content/drive/MyDrive/Personal Data Science Projects/LLM Project/data_token_length.zip'\n",
        "destination_path = ''  # Replace with your desired destination path\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "971aw488A5CM"
      },
      "outputs": [],
      "source": [
        "data_length_before = pd.read_csv(\"/content/data.csv\")\n",
        "data = data.add_column('length before preprocessing', list(data_length_before['length'].values))\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyE-6sUnE2vR"
      },
      "outputs": [],
      "source": [
        "# print(\"Number of Unique Prompts:  \", len(data['system_prompt'].unique()))\n",
        "# print(\"Number of Unique Prompts:  \", len(data[data['length before preprocessing']>512]['system_prompt'].unique()))\n",
        "# unique_prompts = data['system_prompt'].unique()\n",
        "# modified_prompts = list(np.zeros((len(unique_prompts))))\n",
        "# print(\"Original Prompt: \", unique_prompts[0])\n",
        "# modified_prompts[0] = \"\"\n",
        "# print(\"Original Prompt: \", unique_prompts[1])\n",
        "# modified_prompts[1] = \"Generate a detailed and long comprehensive answer\"\n",
        "# print(\"Original Prompt: \", unique_prompts[2])\n",
        "# modified_prompts[2] = \"Generate answer for a kid's understanding\"\n",
        "# print(\"Original Prompt: \", unique_prompts[3])\n",
        "# modified_prompts[3] = \"As Information-finding AI assistant\"\n",
        "# print(\"Original Prompt: \", unique_prompts[4])\n",
        "# modified_prompts[4] = \"Given task by user, complete task faithfully step by step and justify each step.\"\n",
        "# print(\"Original Prompt: \", unique_prompts[5])\n",
        "# modified_prompts[5] = \"Generate answer to given user's questions faithfully, step by step, with justifications\"\n",
        "# print(\"Original Prompt: \", unique_prompts[6])\n",
        "# modified_prompts[6] = \"Provide a detailed answer, eliminating the need for external searches\" # \"Explain like an assistant to a child.\"\n",
        "# print(\"Original Prompt: \", unique_prompts[7])\n",
        "# modified_prompts[7] = \"As highly obedient AI assistant, providing extensive assistance\"\n",
        "# print(\"Original Prompt: \", unique_prompts[8])\n",
        "# modified_prompts[8] = \"As information AI assistant, Generate detailed answers eliminate the need for external searches\"\n",
        "# print(\"Original Prompt: \", unique_prompts[9])\n",
        "# modified_prompts[9] = \"Follow user's task instructions faithfully, justifying step-by-step while answering\"\n",
        "# print(\"Original Prompt: \", unique_prompts[10])\n",
        "# modified_prompts[10] = \"As a teacher, simplify the task, guidelines, and provide steps to answer\"\n",
        "# print(\"Original Prompt: \", unique_prompts[11])\n",
        "# modified_prompts[11] = \"As AI assistant with multilingual expertise explain task and guidelines, and demonstrate guideline utilization\"\n",
        "# print(\"Original Prompt: \", unique_prompts[12])\n",
        "# modified_prompts[12] = \"Break down the task definition into smaller parts, each with an instruction. Provide an example in the format:\\n Part #: Key part of the definition.\\n Usage: Sample response meeting the instruction criteria, with an explanation of why it qualifies.\"\n",
        "# print(\"Original Prompt: \", unique_prompts[13])\n",
        "# modified_prompts[13] = \"Describe given task, give the correct answer(s) to a multiple choice question, and explain why other answers are wrong, child-friendly.\"\n",
        "# print(\"Original Prompt: \", unique_prompts[14])\n",
        "# modified_prompts[14] = \"Describe the task, provide the correct answer(s), and explain why other options are incorrect for a multiple-choice question using additional knowledge.\"\n",
        "# print(\"Original Prompt: \", unique_prompts[15])\n",
        "# modified_prompts[15] = \"Exaplain use of definition to determine the answer.\"\n",
        "# print(\"Original Prompt: \", unique_prompts[16])\n",
        "# modified_prompts[16] = \"As AI assistant, generate detailed answers eliminate the need for external explaination\"\n",
        "# unique_prompts_df = pd.DataFrame()\n",
        "# unique_prompts_df[\"Original Prompt\"] = unique_prompts\n",
        "# unique_prompts_df[\"Modified Prompt\"] = modified_prompts\n",
        "# unique_prompts_df\n",
        "# unique_prompts_df['Original Prompt Length'] = unique_prompts_df['Original Prompt'].apply( lambda text: len(text.split()) )\n",
        "# unique_prompts_df['Modified Prompt Length'] = unique_prompts_df['Modified Prompt'].apply( lambda text: len(text.split()) )\n",
        "# unique_prompts_df\n",
        "# unique_prompts_df.to_csv(\"unique_prompts_df.csv\",index_label=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfkgRJo2CiEd"
      },
      "outputs": [],
      "source": [
        "unique_prompts_df = pd.read_csv(\"/content/drive/MyDrive/Personal Data Science Projects/LLM Project/unique_prompts_df.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "unique_prompts_df.loc[0,\"Original Prompt\"] = \"\"\n",
        "unique_prompts_df.loc[0,\"Modified Prompt\"] = \"\"\n",
        "unique_prompts_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0vqa6dcF7LV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, T5TokenizerFast\n",
        "checkpoint = \"t5-large\"\n",
        "tokenizer = T5TokenizerFast.from_pretrained(checkpoint)\n",
        "model_max_length = 512\n",
        "\n",
        "def tokenize_function(input_sentence):\n",
        "    return tokenizer(input_sentence, truncation=False, padding=False ) # T5TokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6W935u3LwKt"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1GG9eQ7Gq5T"
      },
      "outputs": [],
      "source": [
        "def modify_system_prompt(instance):\n",
        "  # length_before_preprocessing = len(tokenize_function( instance['system_prompt'] + (instance[\"question\"]) )[\"input_ids\"])\n",
        "  if instance['length before preprocessing']>model_max_length:\n",
        "    modified_prompt = unique_prompts_df.loc[unique_prompts_df[\"Original Prompt\"]==\"\", \"Modified Prompt\"].values[0] # unique_prompts_df[unique_prompts_df[\"Original Prompt\"]==instance['system_prompt']][\"Modified Prompt\"].values[0]\n",
        "    tokens_length_after_preprocessing = len(tokenize_function( modified_prompt + \" \\n \" + instance[\"question\"] )[\"input_ids\"])\n",
        "    if tokens_length_after_preprocessing<=model_max_length:\n",
        "      instance[\"system_prompt\"] = modified_prompt\n",
        "    else:\n",
        "      instance[\"system_prompt\"] = \"Drop it\"\n",
        "\n",
        "  return instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E0X9o4pw9iL"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "num_cores = multiprocessing.cpu_count()\n",
        "print(\"Number of CPU cores:\", num_cores)\n",
        "data = data.map(modify_system_prompt, num_proc=num_cores)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt2BbhL7bQXo"
      },
      "outputs": [],
      "source": [
        "data = data.filter(lambda instance, i: instance[\"system_prompt\"] != \"Drop it\", with_indices=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS39EKHZWnmX"
      },
      "outputs": [],
      "source": [
        "# indexes_to_drop = []\n",
        "\n",
        "# for index in tqdm(data.index):\n",
        "#   row = data.iloc[index]\n",
        "\n",
        "#   modified_prompt = unique_prompts_df[unique_prompts_df[\"Original Prompt\"]==row['system_prompt']][\"Modified Prompt\"].values[0]\n",
        "\n",
        "#   if row[\"length before preprocessing\"]>512:\n",
        "#     tokens_length_after_preprocessing = len(tokenizer( modified_prompt + \" [SEP] \" + row[\"question\"] )[\"input_ids\"])\n",
        "\n",
        "#     if tokens_length_after_preprocessing<=512:\n",
        "#       data.loc[index,\"system_prompt\"] = modified_prompt\n",
        "#       data.loc[index,\"length after preprocessing\"] = tokens_length_after_preprocessing\n",
        "\n",
        "#     else:\n",
        "#       indexes_to_drop.append(index)\n",
        "\n",
        "#   else:\n",
        "#       data.loc[index,\"length after preprocessing\"] = row[\"length before preprocessing\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AowlGXZ9WncN"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3887AHo9GTVe"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\", write_permission=True)\n",
        "\n",
        "data.push_to_hub(\"shirsh10mall/First_LLM_Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zDYRsIT5zfw"
      },
      "outputs": [],
      "source": [
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCPj1CD2sW7P"
      },
      "source": [
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZfxE2wEisXce"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import login\n",
        "from tqdm import tqdm\n",
        "from datasets import DatasetDict\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0dmXmtoXsf2L"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ffi64Fnrsf4x"
      },
      "outputs": [],
      "source": [
        "login(token=\"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\", write_permission=True)\n",
        "dataset = load_dataset(\"SKT27182/Preprocessed_OpenOrca\", streaming=True, use_auth_token=True)\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "dataset = concatenate_datasets( [dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]] )\n",
        "dataset = dataset.remove_columns([\"length_before_preprocessing\"])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1y3IsVNMmXiQ"
      },
      "outputs": [],
      "source": [
        "for i, sample in tqdm(enumerate(dataset)):\n",
        "  if i==1:\n",
        "    break\n",
        "  print(sample.keys())\n",
        "  inputs_token = tokenizer(sample[\"system_prompt\"] + \" \" + sample[\"question\"], padding=False, truncation=False).input_ids\n",
        "  responses_tokens = tokenizer(sample[\"response\"], padding=False, truncation=False).input_ids\n",
        "  print( len(inputs_token), len(responses_tokens) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GJbujFWamZCe"
      },
      "outputs": [],
      "source": [
        "def check_response_token_length(sample):\n",
        "\n",
        "  inputs_token = tokenizer(sample[\"system_prompt\"] + \" \" + sample[\"question\"], padding=False, truncation=False).input_ids\n",
        "  responses_tokens = tokenizer(sample[\"response\"], padding=False, truncation=False).input_ids\n",
        "\n",
        "  sample[\"Inputs Token length\"] = len(inputs_token)\n",
        "  sample[\"Response Token length\"] = len(responses_tokens)\n",
        "\n",
        "  # if len(inputs_token) <=512 and len(responses_tokens) <=512:\n",
        "  #    sample[\"drop row or not\"] = \"No\"\n",
        "  # else:\n",
        "  #   sample[\"drop row or not\"] = \"Yes\"\n",
        "\n",
        "  return sample\n",
        "\n",
        "dataset = dataset.map(check_response_token_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HBXFKYLpr4Eh"
      },
      "outputs": [],
      "source": [
        "# dataset = dataset.filter(lambda sample: sample[\"drop row or not\"]==\"No\" , with_indices=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "57GCVM5TmXl0"
      },
      "outputs": [],
      "source": [
        "for i, sample in tqdm(enumerate(dataset)):\n",
        "  if i==2:\n",
        "    break\n",
        "  print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CKQamjZYA60Y"
      },
      "outputs": [],
      "source": [
        "# dataset = dataset.remove_columns([\"drop row or not\",\"Inputs Token length\",\"Response Token length\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hw6-panquNhj"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_generator(lambda: dataset)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "abJPHotjvgT0"
      },
      "outputs": [],
      "source": [
        "# dataset = dataset.train_test_split(test_size=0.2) # ,stratify_by_column=\"system_prompt\")\n",
        "# dataset = dataset.sort(column_names=[\"Inputs Token length\", \"Response Token length\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qEKoJa1Qu6-2"
      },
      "outputs": [],
      "source": [
        "login(token=\"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\", write_permission=True)\n",
        "dataset.push_to_hub(\"shirsh10mall/temp_data_LLM_Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HEWlNRxl3Cy2"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGmBQ-v7hC4-"
      },
      "source": [
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5hJRX98nsf7X"
      },
      "outputs": [],
      "source": [
        "# data = {\"id\":[],\"id_number\":[], \"system_prompt\":[], \"question\":[], \"response\": [], \"length_before_preprocessing\":[], \"text_prompt+ques+resp\":[]}\n",
        "# for i, sample in tqdm(enumerate(dataset[\"train\"])):\n",
        "\n",
        "def modify_column(sample):\n",
        "  Prompt = sample[\"system_prompt\"]\n",
        "  Question = sample[\"question\"]\n",
        "  Response = sample[\"response\"]\n",
        "\n",
        "  final_text = \"{prompt}\\n### Question: {question}\\n### Response: {response}\".format( prompt=Prompt, question=Question, response=Response )\n",
        "\n",
        "  # for key, value in sample.items():\n",
        "  #     data[key].append(value)\n",
        "  sample['text_prompt+ques+resp'] = (final_text)\n",
        "  return sample\n",
        "# data = Dataset.from_dict(data)\n",
        "\n",
        "dataset[\"train\"] = dataset[\"train\"].map(modify_column)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hd61MEZYS513"
      },
      "outputs": [],
      "source": [
        "for i, sample in tqdm(enumerate(dataset[\"train\"])):\n",
        "  if i==1:\n",
        "    break\n",
        "  print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C5JC_qd2CAI"
      },
      "outputs": [],
      "source": [
        "data_dictionary = {}\n",
        "number_of_splits = 20\n",
        "for i in range(0,number_of_splits):\n",
        "  data_chunk = data.shard(num_shards=number_of_splits, index=0)\n",
        "  data_dictionary[\"data_chunk_\"+str(i)] = data_chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQnvJsln3Uui"
      },
      "outputs": [],
      "source": [
        "# data_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyexdWK0sf92"
      },
      "outputs": [],
      "source": [
        "login(token=\"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\", write_permission=True)\n",
        "final_data = DatasetDict(data_dictionary)\n",
        "final_data.push_to_hub(\"shirsh10mall/temp_data_LLM_Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6EpelCH3iQO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}